---
title: "TP Final - Aprendizaje No Supervisado"
author: "Peter Silva"
date: "September 25, 2020"
output: html_document
---
# Se estudiara el dataset de Spotify

#### Cargamos librerias iniciales
```{r}
library(readr)
library(cluster)
library(factoextra)
```

#### Cargamos datos y realizamo analisis exploratorio
```{r}
songs_features = read_csv("songs_features.csv")
names(songs_features)
DF = subset(songs_features, select=-c(id,uri,analysis_url, X1, type, track_href))

boxplot(DF)

library(ggplot2)
library(reshape2)
meltData = melt(DF, id= 5:5)
p = ggplot(meltData, aes(factor(variable), value))
p + geom_boxplot() + facet_wrap(~variable, scale="free")
```

Notamos que las escalas de algunas variables son diferentes. Por ejemplo, hay variables entre cero y uno, y otras por encima de ese valor. Por lo tanto, vamos a escalar las variables que estan fuera del intervalo de cero y uno y luego las reemplamos en el dataset original.

```{r}
DF_subset = subset(DF, select=c('tempo', 'duration_ms', 'loudness')) #subs a normalizar
DF_subset_nuevo = subset(DF, select=c('tempo', 'duration_ms', 'loudness')) #DF con lo normalizado abajo
normalized = function(x){(x-min(x))/(max(x)-min(x))} 

for (i in 1:3) {
  DF_subset_nuevo[i] = normalized(DF_subset[i])  
}

boxplot(DF_subset_nuevo)
DF_trabajado = subset(DF, select= -c(tempo, duration_ms, loudness))
Final_DF = cbind( DF_trabajado, DF_subset_nuevo)

meltData2 = melt(Final_DF, id= 5:5)
p2 = ggplot(meltData2, aes(factor(variable), value)) 
p2 + geom_boxplot() + facet_wrap(~variable, scale="free")

boxplot(Final_DF)

```

Ahora vamos a escalar las categoricas. 

```{r}
DF_subset2 = subset(Final_DF, select=c('time_signature', 'key')) #subs a normalizar. Dejamos afuera mode
DF_subset_nuevo2 = subset(Final_DF, select=c('time_signature', 'key')) #DF con lo normalizado abajo
for (i in 1:2) {
  DF_subset_nuevo2[i] = normalized(DF_subset2[i])  
}
DF_trabajado2 = subset(Final_DF, select= -c(time_signature, key))
Final_DF2 = cbind( DF_trabajado2, DF_subset_nuevo2)
meltData3 = melt(Final_DF2, id= 5:5)
p_2 = ggplot(meltData3, aes(factor(variable), value)) 
p_2 + geom_boxplot() + facet_wrap(~variable, scale="free")
boxplot(Final_DF2)
```


## USANDO K-MEDOIDS 
Dado la aparente presencia de outliers en el dataset final y la poca cantidad de datos, usaremos K-Medoids como primer modelo de segmentacion.

```{r}
Dataset = Final_DF2

fviz_nbclust(x = Dataset, FUNcluster = pam, method = "wss", k.max = 15,
             diss = dist(Dataset, method = "euclidean")) # manhattan
```

Notamos que un posible valor de 'K' podria estar entre 2 y 4. Por ahora, elegimos 4.
```{r}
pam.res = pam(Dataset, k = 4, metric = "euclidean")
```


Sin embargo, al hacer la validacion por silhouette vemos que dos es el numero optimo de clusters:
```{r}
fviz_nbclust(x = Dataset, FUNcluster = pam, method = "silhouette", k.max = 15) +
  labs(title = "Número óptimo de clusters")
```

Por tanto, actualizamos el modelo:
```{r}
pam.res = pam(Dataset, k = 2, metric = "euclidean")
pam_clusters = eclust(x = Dataset, FUNcluster = "pam", k = 2, seed = 123,
                      hc_metric = "euclidean", graph = FALSE)
fviz_silhouette(sil.obj = pam_clusters, print.summary = TRUE, palette = "jco",
                ggtheme = theme_classic()) 
```

Vemos que el silhouette nos queda bajo, pero dentro de los grupos.


Como quedarian segmentada cada variable de acuerdo al modelo usado?
```{r}
library(NbClust)
clustering = as.data.frame(pam.res$clustering)
Consolidado = cbind(Dataset, clustering)
Y = aggregate(. ~ pam.res$clustering, Dataset, mean)
Y2 = cbind(Row.Names = row.names(Y), Y)
Y2['pam.res$clustering'] = NULL
Y2['pam.res$clustering'] = NULL

D = melt(Y2, id='Row.Names')
p5 = ggplot(D, aes(x=factor(Row.Names), y=value, group=factor(Row.Names), colour=factor(Row.Names)))
p5 + geom_point(show.legend = F) + facet_wrap(~variable, scale="free")
```

Analizando esto en terminos de las canciones y artistas:
```{r}
songs_metadata = read_csv("songs_metadata.csv")
names(songs_metadata)
Consolidado2 = cbind(songs_metadata, clustering)
names(Consolidado2)[names(Consolidado2)=='pam.res$clustering'] = 'Cluster'
Consolidado2 = subset(Consolidado2, select=-c(song_id, X1, realese_date, album))
```

veamos que nos muestra el primer cluster de canciones
```{r}
cancionero1 = Consolidado2[Consolidado2$Cluster == '1',]
head(cancionero1, 10)
```
Hagamos lo mismo para el seggundo cluster
```{r}
cancionero2 = Consolidado2[Consolidado2$Cluster == '2',]
head(cancionero2, 10)
```


El primer cluster parece estar conformado por canciones con mayor ritmo, algo que ya habiamos apreciado anteriormente (por ejemplo, se observa mayor danzabilidad y energia). En cambio el segundo cluster tiene una mayor proporción de canciones con un ritmo mas suave como baladas. En este sentido, la metodologia logra clasificar dos grupos de manera apropiada.


## Usando DBSCAN  
A continuacion, probamos otra metodologia que basa la segmentacion en la identificacion de regiones densas. La ventaja de esto es que en caso los cluster no tengan forma esferica, podriamos identificarlos.

Cargamos las librerias a usar
```{r}
library(fpc)
library(dbscan)
```
Comenzaremos evaluando el parametro $epsilon$ que se debe elegir. Este parametro define el radio de la vecindad alrededor de un punto.
```{r}
K2 = 5 #Seteamos el MinPts (numero minimo de vecinos dentro de epsilon) en 5
BaseDatos2 = Final_DF2
kNNdistplot(BaseDatos2, 5)
abline(h = 0.7, lty = 2)
```

A continuacion usamos este valor para generar el modelo. Vemos que DBSCAN tambien logra encontrar dos cluster (identicando outliers).
```{r}
Model_DBS2 = dbscan(BaseDatos2, eps = 0.7, 
                    minPts = K2)
fviz_cluster(Model_DBS2, data = BaseDatos2, 
             stand = FALSE,
             ellipse = FALSE, 
             show.clust.cent = FALSE,
             geom = "point",palette = "jco",
             ggtheme = theme_classic())
```

Para examinar como segmento cada variable generamos el siguiente grafico:
```{r}
clustering_DBSCAN = as.data.frame(Model_DBS2$cluster)
Consolidado_DBScan = cbind(BaseDatos2, clustering_DBSCAN)

Y_DB = aggregate(. ~ Model_DBS2$cluster, BaseDatos2, mean)# el grupo cero son los outliers
Y_DB = Y_DB[-c(1), ]
rownames(Y_DB)[rownames(Y_DB) == "2"] = '1'
rownames(Y_DB)[rownames(Y_DB) == "3"] = '2'

Y2_DB = cbind(Row.Names = row.names(Y_DB), Y_DB) 
Y2_DB['Model_DBS2$cluster'] = NULL


D_DB = melt(Y2_DB, id='Row.Names')
p_DB = ggplot(D_DB, aes(x=factor(Row.Names), y=value, group=factor(Row.Names), colour=factor(Row.Names)))
p_DB + geom_point(show.legend = F) + facet_wrap(~variable, scale="free")
```

A continuacion analizamos las canciones y los artistas que nos quedan al interior de cada grupo:
```{r}
Consolidado2_DB = cbind(songs_metadata, clustering_DBSCAN)
names(Consolidado2_DB)[names(Consolidado2_DB)=='Model_DBS2$cluster'] = 'Cluster'
Consolidado2_DB = subset(Consolidado2_DB, select=-c(song_id, X1, realese_date, album))
```

veamos que nos muestra el el cancionero de este primer cluster
```{r}
cancionero1_DB = Consolidado2_DB[Consolidado2_DB$Cluster == '1',]
head(cancionero1_DB, 10)
```

Ahora, veamos que nos muestra el cancionero del segundo cluster
```{r}
cancionero2_DB = Consolidado2_DB[Consolidado2_DB$Cluster == '2',]
head(cancionero2_DB, 10)
```

Para este modelo, la segmentacion tambien parece separar canciones con mayor ritmo de las que no lo son. Sin embargo, hay canciones dentro del segundo grupo donde a mi parecer deberian ir en el primer grupo (como el caso de Piny it, Black de los Stones). La opinion de un experto en musica podria ayudarnos a entender con mayor detalle este punto.


## Usando el Modelo GMM

Finalmente, probamos un tercer modelo a implementar: Mixtura de Normales. La idea es encontrar el numero de clusters a traves de un modelo formal, considerando que los datos vienen de una distribucion que es un mix de dos o mas cluster. La interesante de este modelo es la probabilidad que le asigna a cada punto de de pertenecer a un determinado cluster.

Vamos a estimar el modelo:  
```{r}
library(mclust)
library(factoextra)
mixmodel = densityMclust(Final_DF2)#Estimamos las densidades
summary(mixmodel)
fviz_mclust(mixmodel, 'BIC', palette = 'jco')
```

El algoritmo encuentra cuatro clusters, donde el modelo elegido por el algoritmo es de la forma VEV (distribucion elipsoidal, volumen variable, forma igual y orientacion variable).

Veamos como realizo la clasificacion y la incertidumbre asociada.
```{r}
fviz_mclust(mixmodel, 'uncertainty', palette='jco')
fviz_mclust(mixmodel, 'classification', palette='jco',geom = 'point', pointsize = 1) #los puntos mas gruesos corresponden a una mayor incertidumbre
```

Veamos las probabilidades, las cantidades de datos por cluster y los parametros del modelo
```{r}
table(mixmodel$classification)#Para ver la cantidd de datos dentro de cada cluster
round(table(mixmodel$classification)/nrow(Final_DF),3)#"Proporción de observaciones en cada cluster
round(mixmodel$parameters$pro,3)#"Las famosas pi de las diaps"
```

Finalmente, evaluemos la performance del modelo usando el indicador de Silhoutte:
```{r}
si2 = silhouette(mixmodel$classification, dist(Final_DF2, "canberra"))
fviz_silhouette(sil.obj = si2, print.summary = TRUE, palette = "jco",
                ggtheme = theme_classic())
```

Como vemos, el indicador no es tan bueno. Dos grupos tienen silhoutte negativo y los otros dos, aunque positivos, no tienen valores tan altos.





