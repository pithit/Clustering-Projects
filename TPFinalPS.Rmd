# Spotify Dataset

# upload data
```{r}
library(readr)
library(cluster)
library(factoextra)
```

# Exploratory analysis
```{r}
songs_features = read_csv("songs_features.csv")
names(songs_features)
DF = subset(songs_features, select=-c(id,uri,analysis_url, X1, type, track_href))

boxplot(DF)

library(ggplot2)
library(reshape2)
meltData = melt(DF, id= 5:5)
p = ggplot(meltData, aes(factor(variable), value))
p + geom_boxplot() + facet_wrap(~variable, scale="free")
```

#since there are features in diffetent scales we must normalize them to the 1 - 0 interval

```{r}
DF_subset = subset(DF, select=c('tempo', 'duration_ms', 'loudness')) #subs a normalizar
DF_subset_nuevo = subset(DF, select=c('tempo', 'duration_ms', 'loudness')) #DF con lo normalizado abajo
normalized = function(x){(x-min(x))/(max(x)-min(x))} 

for (i in 1:3) {
  DF_subset_nuevo[i] = normalized(DF_subset[i])  
}

boxplot(DF_subset_nuevo)
DF_trabajado = subset(DF, select= -c(tempo, duration_ms, loudness))
Final_DF = cbind( DF_trabajado, DF_subset_nuevo)

meltData2 = melt(Final_DF, id= 5:5)
p2 = ggplot(meltData2, aes(factor(variable), value)) 
p2 + geom_boxplot() + facet_wrap(~variable, scale="free")

boxplot(Final_DF)

```

#we do the same with categorical features

```{r}
DF_subset2 = subset(Final_DF, select=c('time_signature', 'key')) #subs a normalizar. Dejamos afuera mode
DF_subset_nuevo2 = subset(Final_DF, select=c('time_signature', 'key')) #DF con lo normalizado abajo
for (i in 1:2) {
  DF_subset_nuevo2[i] = normalized(DF_subset2[i])  
}
DF_trabajado2 = subset(Final_DF, select= -c(time_signature, key))
Final_DF2 = cbind( DF_trabajado2, DF_subset_nuevo2)
meltData3 = melt(Final_DF2, id= 5:5)
p_2 = ggplot(meltData3, aes(factor(variable), value)) 
p_2 + geom_boxplot() + facet_wrap(~variable, scale="free")
boxplot(Final_DF2)
```


## K-MEDOIDS approach
#Apparently, there are some outliers in the data. So, K-Medoids is a good starting point

```{r}
Dataset = Final_DF2

fviz_nbclust(x = Dataset, FUNcluster = pam, method = "wss", k.max = 15,
             diss = dist(Dataset, method = "euclidean")) # manhattan
```

#Must to find the number of cluster (guess with 4)
```{r}
pam.res = pam(Dataset, k = 4, metric = "euclidean")
```

#However, silhouette assess shows 3 cluster
```{r}
fviz_nbclust(x = Dataset, FUNcluster = pam, method = "silhouette", k.max = 15) +
  labs(title = "Número óptimo de clusters")
```

#Updating the model
```{r}
pam.res = pam(Dataset, k = 2, metric = "euclidean")
pam_clusters = eclust(x = Dataset, FUNcluster = "pam", k = 2, seed = 123,
                      hc_metric = "euclidean", graph = FALSE)
fviz_silhouette(sil.obj = pam_clusters, print.summary = TRUE, palette = "jco",
                ggtheme = theme_classic()) 
```



#Here are the clusters per feature
```{r}
library(NbClust)
clustering = as.data.frame(pam.res$clustering)
Consolidado = cbind(Dataset, clustering)
Y = aggregate(. ~ pam.res$clustering, Dataset, mean)
Y2 = cbind(Row.Names = row.names(Y), Y)
Y2['pam.res$clustering'] = NULL
Y2['pam.res$clustering'] = NULL

D = melt(Y2, id='Row.Names')
p5 = ggplot(D, aes(x=factor(Row.Names), y=value, group=factor(Row.Names), colour=factor(Row.Names)))
p5 + geom_point(show.legend = F) + facet_wrap(~variable, scale="free")
```

#Let see  what songs are artist are in each group
```{r}
songs_metadata = read_csv("songs_metadata.csv")
names(songs_metadata)
Consolidado2 = cbind(songs_metadata, clustering)
names(Consolidado2)[names(Consolidado2)=='pam.res$clustering'] = 'Cluster'
Consolidado2 = subset(Consolidado2, select=-c(song_id, X1, realese_date, album))
```

#first cluster
```{r}
cancionero1 = Consolidado2[Consolidado2$Cluster == '1',]
head(cancionero1, 10)
```
#second one
```{r}
cancionero2 = Consolidado2[Consolidado2$Cluster == '2',]
head(cancionero2, 10)
```


## Usando DBSCAN  
#Nor, we play with a density model
#upload libraries
```{r}
library(fpc)
library(dbscan)
```
#checking epsilon parameter
```{r}
K2 = 5 #Seteamos el MinPts (numero minimo de vecinos dentro de epsilon) en 5
BaseDatos2 = Final_DF2
kNNdistplot(BaseDatos2, 5)
abline(h = 0.7, lty = 2)
```

#generating the model. Two clusters too.
```{r}
Model_DBS2 = dbscan(BaseDatos2, eps = 0.7, 
                    minPts = K2)
fviz_cluster(Model_DBS2, data = BaseDatos2, 
             stand = FALSE,
             ellipse = FALSE, 
             show.clust.cent = FALSE,
             geom = "point",palette = "jco",
             ggtheme = theme_classic())
```

#cheking clusters by variable
```{r}
clustering_DBSCAN = as.data.frame(Model_DBS2$cluster)
Consolidado_DBScan = cbind(BaseDatos2, clustering_DBSCAN)

Y_DB = aggregate(. ~ Model_DBS2$cluster, BaseDatos2, mean)# el grupo cero son los outliers
Y_DB = Y_DB[-c(1), ]
rownames(Y_DB)[rownames(Y_DB) == "2"] = '1'
rownames(Y_DB)[rownames(Y_DB) == "3"] = '2'

Y2_DB = cbind(Row.Names = row.names(Y_DB), Y_DB) 
Y2_DB['Model_DBS2$cluster'] = NULL


D_DB = melt(Y2_DB, id='Row.Names')
p_DB = ggplot(D_DB, aes(x=factor(Row.Names), y=value, group=factor(Row.Names), colour=factor(Row.Names)))
p_DB + geom_point(show.legend = F) + facet_wrap(~variable, scale="free")
```

#cheking clusters
```{r}
Consolidado2_DB = cbind(songs_metadata, clustering_DBSCAN)
names(Consolidado2_DB)[names(Consolidado2_DB)=='Model_DBS2$cluster'] = 'Cluster'
Consolidado2_DB = subset(Consolidado2_DB, select=-c(song_id, X1, realese_date, album))
```

#First cluster
```{r}
cancionero1_DB = Consolidado2_DB[Consolidado2_DB$Cluster == '1',]
head(cancionero1_DB, 10)
```

#second one
```{r}
cancionero2_DB = Consolidado2_DB[Consolidado2_DB$Cluster == '2',]
head(cancionero2_DB, 10)
```

## USing GMM 

```{r}
library(mclust)
library(factoextra)
mixmodel = densityMclust(Final_DF2)#Estimamos las densidades
summary(mixmodel)
fviz_mclust(mixmodel, 'BIC', palette = 'jco')
```

#the algo finds 4 clusters
```{r}
fviz_mclust(mixmodel, 'uncertainty', palette='jco')
fviz_mclust(mixmodel, 'classification', palette='jco',geom = 'point', pointsize = 1) #los puntos mas gruesos corresponden a una mayor incertidumbre
```

#cheking the probabilities 
```{r}
table(mixmodel$classification)#Para ver la cantidd de datos dentro de cada cluster
round(table(mixmodel$classification)/nrow(Final_DF),3)#"Proporción de observaciones en cada cluster
round(mixmodel$parameters$pro,3)#"Las famosas pi de las diaps"
```

#However, the Silhoutte shows a poor performance
```{r}
si2 = silhouette(mixmodel$classification, dist(Final_DF2, "canberra"))
fviz_silhouette(sil.obj = si2, print.summary = TRUE, palette = "jco",
                ggtheme = theme_classic())
```

#So, I decide to keep two clusters.





